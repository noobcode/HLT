{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# custom\n",
    "from analize_text import get_sentenceID\n",
    "from paths import *\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# scikit learn\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# keras\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from feature_transformer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataframes of sentences and entities\n",
    "sentences_df_train = pd.read_csv(SENTENCE_PATH_train)\n",
    "entities_df_train = pd.read_csv(ENTITY_PATH_train)\n",
    "\n",
    "sentences_df_test1 = pd.read_csv(SENTENCE_PATH_test1)\n",
    "entities_df_test1 = pd.read_csv(ENTITY_PATH_test1)\n",
    "\n",
    "sentences_df_test2 = pd.read_csv(SENTENCE_PATH_test2)\n",
    "entities_df_test2 = pd.read_csv(ENTITY_PATH_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenating training and test data for the word2vec training!\n",
    "sentences_df = pd.concat([sentences_df_train,\n",
    "                          sentences_df_test1,\n",
    "                          sentences_df_test2]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "entities_df = pd.concat([entities_df_train,\n",
    "                         entities_df_test1,\n",
    "                         entities_df_test2]).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Entities dataframe')\n",
    "entities_df_train.head()\n",
    "#entities_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sentences dataframe')\n",
    "sentences_df_train.head()\n",
    "#sentences_df_test2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load label dictionary {sentenceID: [ 'B', 'I', ..., 'O'] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_dict_path = os.path.join(ROOT_DIR, 'Train', 'bio_labels')\n",
    "label_dict = np.load(label_dict_path + '.npy').item()\n",
    "\n",
    "sentenceIDs = label_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get sentences containing at least an entity \n",
    "#sentences = [sentences_df[sentences_df.sentenceID == sentenceID]['sentenceText'].values[0] \n",
    "#             for sentenceID in sentenceIDs]\n",
    "\n",
    "sentences = [row['sentenceText'] for index, row in sentences_df.iterrows()]\n",
    "\n",
    "# remove duplicates from sentence list (sentences with e.g. 2 entities appeared twice)\n",
    "sentences = list(set(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### WTF ### happens iterating from sentences_df instead of indexing by entities \n",
    "dd = sentences_df['sentenceText']\n",
    "for i, sentence in zip(range(len(dd)), dd.values):\n",
    "    if not isinstance(sentence, str):\n",
    "        print(i, True, sentence)\n",
    "        #break # remove this to see all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just check nothing is wrong (it should not print anything)\n",
    "for s in sentences:\n",
    "    if not isinstance(s, str):\n",
    "        print(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "len(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemma (todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### EXAMPLE STEM + POS ####\n",
    "# POS could differ slightly when applied to the stemmed version or not\n",
    "# TODO: try which one performs better\n",
    "stemmer = EnglishStemmer()\n",
    "s = ['interaction', 'between', 'cimetidine', 'and', 'warfarin', 'could', 'be', 'dangerous']\n",
    "\n",
    "print ('stemmed version:\\n')\n",
    "stemmed_s = [stemmer.stem(w) for w in s]\n",
    "print (stemmed_s)\n",
    "\n",
    "print ('\\noriginal pos tags:\\n')\n",
    "print(pos_tag(s))\n",
    "\n",
    "print ('\\nstemmed pos tags:\\n')\n",
    "print(pos_tag(stemmed_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences_pos = pos_tag_sents(tokenized_sentences, tagset=None) # tagset = None, 'universal', 'wsj', 'brown'\n",
    "\n",
    "# concatenate the part of speach to each word (e.g. cat_NN)\n",
    "tokenized_sentences_pos = [ [w + '_' + pos for w, pos in s ] for s in tokenized_sentences_pos]\n",
    "len(tokenized_sentences_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 20\n",
    "model = Word2Vec(tokenized_sentences_pos, size=vector_size, window=5, min_count=1, workers=cpu_count(), compute_loss=True)\n",
    "model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "print ('latest loss:', model.get_latest_training_loss())\n",
    "\n",
    "# save embeddings and delete model\n",
    "model.save(\"../word_vectors\")\n",
    "#model = Word2Vec.load('../word_vectors')\n",
    "word_vectors = model.wv\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_vectors[\"conduction_NN\"])\n",
    "print(word_vectors['105_CD'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(df):\n",
    "    X = np.array([]).reshape(0,vector_size)\n",
    "    words = np.array([])\n",
    "    Y = np.array([])\n",
    "    for sentenceID, labels in label_dict.items():\n",
    "        if df[df.sentenceID == sentenceID].empty:\n",
    "            #print('empty')\n",
    "            continue\n",
    "        else: \n",
    "            sentence = df[df.sentenceID == sentenceID]['sentenceText'].values[0] \n",
    "        tok_sentence = word_tokenize(sentence)\n",
    "        tok_sentence_pos = [ word + '_' + pos for word, pos in pos_tag(tok_sentence, tagset=None)]\n",
    "\n",
    "        for word, label in zip(tok_sentence_pos, labels):\n",
    "            word_vector = word_vectors[word]\n",
    "            X = np.vstack((X, word_vector))\n",
    "            Y = np.append(Y, label)       \n",
    "            words = np.append(words, word)\n",
    "            #print(word)\n",
    "    return X, Y, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create X_train, Y_train\n",
    "X_train = np.array([]).reshape(0,vector_size)\n",
    "Y_train = np.array([])\n",
    "words_train = np.array([])\n",
    "\n",
    "X_test1 = np.array([]).reshape(0,vector_size)\n",
    "Y_test1 = np.array([])\n",
    "words_test1 = np.array([])\n",
    "\n",
    "#X_test2 = np.array([]).reshape(0,vector_size)\n",
    "#Y_test2 = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = load_data(sentences_df_train)\n",
    "X_test1, Y_test1 = load_data(sentences_df_test1)\n",
<<<<<<< HEAD
    "#X_test2, Y_test2 = load_data(sentences_df_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
=======
    "#X_test2, Y_test2 = load_data(sentences_df_test2)\n",
    "\n",
>>>>>>> cd32fae80f70ca579fa6abb726569dc9a085854b
    "print (X_train.shape, Y_train.shape)\n",
    "print (X_test1.shape, Y_test1.shape)\n",
    "#print (X_test2.shape, Y_test2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode labels\n",
    "- Convert labels from B-I-O to $0, 1, 2$ for SVM\n",
    "- Convert labels from B-I-O to $[1 0 0, 0 1 0, 0 0 1]$ for ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode class values as integers = B-I-O -> 0-1-2\n",
    "encoder = LabelEncoder()\n",
    "encoded_Y = encoder.fit_transform(Y_train)\n",
    "Y_train = encoded_Y\n",
    "# convert integers to one-hot encoding\n",
    "Y_train_one_hot = np_utils.to_categorical(encoded_Y) # SVM does not need one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating feature transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Embedder(FeatureTransformer):\n",
    "  # returns embedding for each word\n",
    "  def transform(self, X, y=None, **fit_params):\n",
    "    return np.array([word_vectors[x] for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb = Embedder()\n",
    "e = emb.transform(['conduction_NN', words_train[7]])\n",
    "print(e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Capitalizer(FeatureTransformer):\n",
    "    # return 1 if the word is capitalized\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return np.array([1 if x[0].isupper() else 0 for x in X]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cap = Capitalizer()\n",
    "c = cap.transform(['conduction_NN', words_train[7]])\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "clf = LinearSVC()\n",
    "\n",
    "pipe = FeatureUnion([\n",
    "    ('emb', Embedder()),\n",
    "    ('cap', Capitalizer()),\n",
    "])\n",
    "\n",
    "print(words_train.reshape(-1, 1).shape)\n",
    "data = pipe.transform(words_train.reshape(-1, 1))\n",
    "print(data.shape)\n",
    "#clf.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "word_tokens = word_tokenize(example_sent)\n",
    " \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    " \n",
    "filtered_sentence = []\n",
    " \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    " \n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PunctuationRemoval(FeatureTransformer):\n",
    "    def transform():\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "experiment with:\n",
    "- original data\n",
    "- MinMaxScaler\n",
    "- Standardizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#min_max_scaler = MinMaxScaler()\n",
    "#X_train = min_max_scaler.fit_transform(X_train)\n",
    "\n",
    "#standard_scaler = StandardScaler()\n",
    "#X_train = standard_scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split train / validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split train validation (SVM)\n",
    "train_perc = 0.9\n",
    "train_size = int(len(X_train) * train_perc)\n",
    "\n",
    "X_tr, X_vl = X_train[:train_size,:], X_train[train_size:,:]\n",
    "Y_tr, Y_vl = Y_train[:train_size], Y_train[train_size:]\n",
    "\n",
    "print (\"X train/validation shapes:\", X_tr.shape, X_vl.shape)\n",
    "print (\"Y train/validation shapes (svm):\", Y_tr.shape, Y_vl.shape)\n",
    "\n",
    "# split train validatioin (NN)\n",
    "Y_tr_nn, Y_vl_nn = Y_train_one_hot[:train_size], Y_train_one_hot[train_size:]\n",
    "print (\"Y train/validation shapes: (ann): \", Y_tr_nn.shape, Y_vl_nn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# one-vs-all classifier\n",
    "model = svm.SVC(kernel='rbf', \n",
    "                C=1.0,\n",
    "                class_weight=None,\n",
    "                gamma='auto',\n",
    "                #penalty='l2',\n",
    "                #loss='squared_hinge',\n",
    "                tol=0.001, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_tr, Y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute metrics\n",
    "- accuracy\n",
    "- f1-score (micro, macro, weighted)\n",
    "- precision (micro, macro, weighted)\n",
    "- recall (micro, macro, weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_vl)\n",
    "\n",
    "# micro:\n",
    "# computes metrics globally - considering total number of TP, FP, FN\n",
    "f1_micro = f1_score(Y_vl, predictions, average='micro')\n",
    "recall_micro = recall_score(Y_vl, predictions, average='micro')\n",
    "precision_micro = precision_score(Y_vl, predictions, average='micro')\n",
    "\n",
    "# macro:\n",
    "# compute metrics for each label, then unweighted average them - does not take class imbalance into account\n",
    "f1_macro = f1_score(Y_vl, predictions, average='macro')\n",
    "recall_macro = recall_score(Y_vl, predictions, average='macro')\n",
    "precision_macro = precision_score(Y_vl, predictions, average='macro')\n",
    "\n",
    "# weighted:\n",
    "# as macro, but taking into account true class frequencies for each label\n",
    "f1_weighted = f1_score(Y_vl, predictions, average='weighted')\n",
    "recall_weighted = recall_score(Y_vl, predictions, average='weighted')\n",
    "precision_weighted = precision_score(Y_vl, predictions, average='weighted')\n",
    "\n",
    "accuracy = accuracy_score(Y_vl, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('SVM validation accuracy:', round(accuracy, 4))\n",
    "print ('-'*20)\n",
    "print ('SVM validation f1-score (micro):', round(f1_micro, 4))\n",
    "print ('SVM validation precision (micro):', round(precision_micro, 4))\n",
    "print ('SVM validation recall (micro):', round(recall_micro, 4))\n",
    "print ('-'*20)\n",
    "print ('SVM validation f1-score (macro):', round(f1_macro, 4))\n",
    "print ('SVM validation precision (macro):', round(precision_macro, 4))\n",
    "print ('SVM validation recall (macro):', round(recall_macro, 4))\n",
    "print ('-'*20)\n",
    "print ('SVM validation f1-score (weighted):', round(f1_weighted, 4))\n",
    "print ('SVM validation precision (weighted):', round(precision_weighted, 4))\n",
    "print ('SVM validation recall (weighted):', round(recall_weighted, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from sklearn import preprocessing\n",
    "from keras.optimizers import *\n",
    "from keras.initializers import *\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_inputs = X_train.shape[1] # size of a vector\n",
    "num_outputs = 3 # b-i-o tags\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=64, input_shape=(num_inputs,), activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(units=num_outputs, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training callbacks\n",
    "- F1-score\n",
    "- Precision\n",
    "- Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class Metrics(Callback):\n",
    "\n",
    "    def __init__(self, x_val, y_val):\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s_micro = []\n",
    "        self.val_recalls_micro = []\n",
    "        self.val_precisions_micro = []\n",
    "        self.val_f1s_macro = []\n",
    "        self.val_recalls_macro = []\n",
    "        self.val_precisions_macro = []\n",
    "        self.val_f1s_weighted = []\n",
    "        self.val_recalls_weighted = []\n",
    "        self.val_precisions_weighted = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = np.argmax((np.asarray(self.model.predict(self.x_val))), axis=1)\n",
    "        val_targ = np.argmax(self.y_val, axis=1)\n",
    "        # micro\n",
    "        f1_micro = f1_score(val_targ, val_predict, average='micro')\n",
    "        recall_micro = recall_score(val_targ, val_predict, average='micro')\n",
    "        precision_micro = precision_score(val_targ, val_predict, average='micro')\n",
    "        # macro\n",
    "        f1_macro = f1_score(val_targ, val_predict, average='macro')\n",
    "        recall_macro = recall_score(val_targ, val_predict, average='macro')\n",
    "        precision_macro = precision_score(val_targ, val_predict, average='macro')\n",
    "        # weighted\n",
    "        f1_weighted = f1_score(val_targ, val_predict, average='weighted')\n",
    "        recall_weighted = recall_score(val_targ, val_predict, average='weighted')\n",
    "        precision_weighted = precision_score(val_targ, val_predict, average='weighted')\n",
    "        \n",
    "        # append metrics to access them later\n",
    "        # micro\n",
    "        self.val_f1s_micro.append(f1_micro)\n",
    "        self.val_recalls_micro.append(recall_micro)\n",
    "        self.val_precisions_micro.append(precision_micro)\n",
    "        # macro\n",
    "        self.val_f1s_macro.append(f1_macro)\n",
    "        self.val_recalls_macro.append(recall_macro)\n",
    "        self.val_precisions_macro.append(precision_macro)\n",
    "        # weighted\n",
    "        self.val_f1s_weighted.append(f1_weighted)\n",
    "        self.val_recalls_weighted.append(recall_weighted)\n",
    "        self.val_precisions_weighted.append(precision_weighted)\n",
    "        \n",
    "        print (\" — val_f1_micro: %.4f — val_precision_micro: %.4f — val_recall_micro: %.4f\" % (f1_micro, precision_micro, recall_micro))\n",
    "        print (\" — val_f1_macro: %.4f — val_precision_macro: %.4f — val_recall_macro: %.4f\" % (f1_macro, precision_macro, recall_macro))\n",
    "        print (\" — val_f1_weighted: %.4f — val_precision_weighted: %.4f — val_recall_weighted: %.4f\" % (f1_micro, precision_micro, recall_micro))\n",
    "        return\n",
    "\n",
    "    \n",
    "metrics_callback = Metrics(x_val=X_vl, y_val=Y_vl_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "history = model.fit(X_tr, Y_tr_nn, \n",
    "                    epochs=epochs, \n",
    "                    shuffle=True, verbose=1, \n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(X_vl, Y_vl_nn),\n",
    "                    callbacks=[metrics_callback])"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(precision_score)"
   ]
  },
  {
=======
>>>>>>> cd32fae80f70ca579fa6abb726569dc9a085854b
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='tr loss', linestyle='--', marker='o')\n",
    "plt.plot(history.history['val_loss'], label='vl loss', linestyle='-', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cathegorical cross-entropy')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['acc'], label='tr accuracy', linestyle='--', marker='o')\n",
    "plt.plot(history.history['val_acc'], label='vl accuracy', linestyle='-', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of F1-score, Precision and Recall (micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics_callback.val_f1s_micro, label='f1-score', marker='o', alpha=0.7)\n",
    "plt.plot(metrics_callback.val_precisions_micro, label='precision', marker='D', alpha=0.7)\n",
    "plt.plot(metrics_callback.val_recalls_micro, label='recall', marker='*', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Micro-averaged performance')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of F1-score, Precision and Recall (macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(metrics_callback.val_f1s_macro, label='f1-score', marker='o', alpha=0.7)\n",
    "plt.plot(metrics_callback.val_precisions_macro, label='precision', marker='D', alpha=0.7)\n",
    "plt.plot(metrics_callback.val_recalls_macro, label='recall', marker='*', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Macro-averaged performance')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of F1-score, Precision and Recall (weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(metrics_callback.val_f1s_weighted, label='f1-score', marker='o', alpha=0.7)\n",
    "plt.plot(metrics_callback.val_precisions_weighted, label='precision', marker='D', alpha=0.7)\n",
    "plt.plot(metrics_callback.val_recalls_weighted, label='recall', marker='*', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Weighted-averaged performance')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "IdSentence|startOffset-endOffset|text|null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_spans(txt):\n",
    "    token_offset = []\n",
    "    tokens = nltk.word_tokenize(txt)\n",
    "    offset = 0\n",
    "    for token in tokens:\n",
    "        offset = txt.find(token, offset)\n",
    "        token_offset.append((token, offset, offset+len(token)-1))\n",
    "        offset += len(token)\n",
    "    return tokens, token_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_end_entity_index(labels_list, current_word_index):\n",
    "    end_entity_index = current_word_index\n",
    "    for i in range(current_word_index + 1, len(labels_list)):\n",
    "        if labels_list[i] == 1: # if label == I\n",
    "            end_entity_index += 1\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    return end_entity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_string = ''\n",
    "\n",
    "for index, row in sentences_df_test1.iterrows():\n",
    "    sentenceId = row['sentenceID']\n",
    "    sentenceText = row['sentenceText']\n",
    "    # 1. tokenize sentence\n",
    "    tok_sentence, token_offset = token_spans(sentenceText)\n",
    "    # 2. add part of speech\n",
    "    tok_sentence_pos = [ word + '_' + pos for word, pos in pos_tag(tok_sentence, tagset=None)]\n",
    "    # 3. get word vectors, predict and write output line\n",
    "    vectors_to_predict = np.array([]).reshape(0, vector_size)\n",
    "    for word in tok_sentence_pos:\n",
    "        vector = word_vectors[word]\n",
    "        vectors_to_predict = np.vstack((vectors_to_predict, vector))\n",
    "    # 4. predict\n",
    "    predictions = model.predict(vectors_to_predict)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    # 5. generate output\n",
    "    for i in range(len(predicted_labels)):\n",
    "        if predicted_labels[i] == 0:\n",
    "            end_entity_index = find_end_entity_index(predicted_labels, i)\n",
    "            start = token_offset[i][1]\n",
    "            end = token_offset[end_entity_index][2]\n",
    "            output_string += sentenceId + '|' + str(start) + '-' + str(end) + '|' + sentenceText[start:end+1] + '|null\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_task_1 = '../results/task9.1_GROUP_1.txt'\n",
    "with open(output_file_task_1, \"w\") as out_file:\n",
    "    out_file.write(output_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
