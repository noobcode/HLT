{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# to make nbs importable\n",
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# custom\n",
    "from analize_text import get_sentenceID\n",
    "from paths import *\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading sentences from structured data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6832 664 1299\n"
     ]
    }
   ],
   "source": [
    "# read dataframes of sentences and entities\n",
    "\n",
    "# TRAIN SET\n",
    "sentences_df_train = pd.read_csv(SENTENCE_PATH_train)\n",
    "entities_df_train = pd.read_csv(ENTITY_PATH_train)\n",
    "\n",
    "#TEST SET\n",
    "sentences_df_test1 = pd.read_csv(SENTENCE_PATH_test1)\n",
    "entities_df_test1 = pd.read_csv(ENTITY_PATH_test1)\n",
    "\n",
    "#TEST2 SET\n",
    "sentences_df_test2 = pd.read_csv(SENTENCE_PATH_test2)\n",
    "entities_df_test2 = pd.read_csv(ENTITY_PATH_test2)\n",
    "\n",
    "print(len(sentences_df_train), len(sentences_df_test1), len(sentences_df_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Concatenating training and test data for the word2vec training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences_df = pd.concat([sentences_df_train,\n",
    "                          sentences_df_test1,\n",
    "                          sentences_df_test2]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "entities_df = pd.concat([entities_df_train,\n",
    "                         entities_df_test1,\n",
    "                         entities_df_test2]).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences dataframe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentenceID</th>\n",
       "      <th>sentenceText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DDI-DrugBank.d281.s0</td>\n",
       "      <td>Probenecid may decrease renal tubular secretio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DDI-DrugBank.d281.s1</td>\n",
       "      <td>Drug/Laboratory Test Interactions A false posi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DDI-DrugBank.d281.s2</td>\n",
       "      <td>Positive direct and indirect antiglobulin (Coo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DDI-DrugBank.d281.s3</td>\n",
       "      <td>these may also occur in neonates whose mothers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DDI-DrugBank.d384.s0</td>\n",
       "      <td>Interactions for vitamin D analogues (Vitamin ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             sentenceID                                       sentenceText\n",
       "0  DDI-DrugBank.d281.s0  Probenecid may decrease renal tubular secretio...\n",
       "1  DDI-DrugBank.d281.s1  Drug/Laboratory Test Interactions A false posi...\n",
       "2  DDI-DrugBank.d281.s2  Positive direct and indirect antiglobulin (Coo...\n",
       "3  DDI-DrugBank.d281.s3  these may also occur in neonates whose mothers...\n",
       "4  DDI-DrugBank.d384.s0  Interactions for vitamin D analogues (Vitamin ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Sentences dataframe')\n",
    "sentences_df_train.head()\n",
    "#sentences_df_test2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entities dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities dataframe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entityID</th>\n",
       "      <th>name</th>\n",
       "      <th>position</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DDI-DrugBank.d281.s0.e0</td>\n",
       "      <td>Probenecid</td>\n",
       "      <td>0-9</td>\n",
       "      <td>drug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DDI-DrugBank.d281.s0.e1</td>\n",
       "      <td>cephalosporins</td>\n",
       "      <td>51-64</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DDI-DrugBank.d281.s0.e2</td>\n",
       "      <td>cephalosporin</td>\n",
       "      <td>132-144</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DDI-DrugBank.d281.s3.e0</td>\n",
       "      <td>cephalosporins</td>\n",
       "      <td>56-69</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DDI-DrugBank.d384.s0.e0</td>\n",
       "      <td>vitamin D analogues</td>\n",
       "      <td>17-35</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  entityID                 name position   type\n",
       "0  DDI-DrugBank.d281.s0.e0           Probenecid      0-9   drug\n",
       "1  DDI-DrugBank.d281.s0.e1       cephalosporins    51-64  group\n",
       "2  DDI-DrugBank.d281.s0.e2        cephalosporin  132-144  group\n",
       "3  DDI-DrugBank.d281.s3.e0       cephalosporins    56-69  group\n",
       "4  DDI-DrugBank.d384.s0.e0  vitamin D analogues    17-35  group"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Entities dataframe')\n",
    "entities_df_train.head()\n",
    "#entities_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load label dictionary {sentenceID: [ 'B', 'I', ..., 'O'] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words: 192766\n"
     ]
    }
   ],
   "source": [
    "label_dict_path = os.path.join(ROOT_DIR, 'Train', 'bio_labels')\n",
    "label_dict = np.load(label_dict_path + '.npy').item()\n",
    "\n",
    "sentenceIDs = label_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8492\n"
     ]
    }
   ],
   "source": [
    "sentences = [row['sentenceText'] for index, row in sentences_df.iterrows()]\n",
    "# remove duplicates from sentence list (sentences with e.g. 2 entities appeared twice)\n",
    "sentences = list(set(sentences))\n",
    "#print(sentences)\n",
    "print(len(sentences))\n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming + POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:  ['interaction', 'between', 'cimetidine', 'and', 'warfarin', 'could', 'be', 'dangerous']\n",
      "stemmed:   [('interact', [('interaction', 'NN')]), ('between', [('between', 'IN')]), ('cimetidin', [('cimetidine', 'NN')]), ('and', [('and', 'CC')]), ('warfarin', [('warfarin', 'NN')]), ('could', [('could', 'MD')]), ('be', [('be', 'VB')]), ('danger', [('dangerous', 'JJ')])]\n",
      "Conclusion: It stemms the drug names too.\n"
     ]
    }
   ],
   "source": [
    "# POS needs to be extracted before stemming appended afterwards\n",
    "# TODO: try which one performs better\n",
    "stemmer = EnglishStemmer()\n",
    "s = ['interaction', 'between', 'cimetidine', 'and', 'warfarin', 'could', 'be', 'dangerous']\n",
    "\n",
    "print ('original: ', s)\n",
    "print('stemmed:  ', [(stemmer.stem(w), pos_tag([w])) for w in s])\n",
    "print('Conclusion: It stemms the drug names too.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending POS tags\n",
    "- tokens will have the form: stemmed-word_POS if argument stem=True\n",
    "- tokens will have the form: word_POS         if argument stem=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "punctuation = [\".\",\",\", \":\", \";\", \"!\", \"?\", \"(\", \")\", \"%\", \"[\",\"]\", \"-\", \"e.g.\"]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def isNumber(inputString):\n",
    "    try:\n",
    "        int(inputString)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def tokenize(sentences, stem=True, POS=True, remove_stopwords=False, remove_punctuation=True, lower=False, remove_nums=False):\n",
    "    tokenized = [word_tokenize(sentence) for sentence in sentences]\n",
    "    #print(tokenized)\n",
    "    if lower:\n",
    "        tokenized = [[w.lower() for w in s] for s in tokenized]\n",
    "    if remove_punctuation:\n",
    "        tokenized = [[w for w in s if w not in punctuation] for s in tokenized]\n",
    "    if remove_nums:\n",
    "        tokenized = [[w for w in s if not isNumber(w)] for s in tokenized]\n",
    "        #print(tokenized)\n",
    "    if remove_stopwords:\n",
    "        tokenized = [[w for w in s if w.lower() not in stop_words] for s in tokenized]\n",
    "        #print(tokenized)\n",
    "    if POS:\n",
    "        tokenized = pos_tag_sents(tokenized, tagset=None)\n",
    "        #print(tokenized)\n",
    "    if stem and POS:\n",
    "        tokenized = [ [stemmer.stem(w) + '_' + pos for w, pos in s ] for s in tokenized]\n",
    "    if stem and not POS:\n",
    "        tokenized = [ [stemmer.stem(w) for w in s ] for s in tokenized]\n",
    "        #filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('first', 'RB'), ('dinner', 'NN'), ('barcelona', 'NN'), ('blast', 'NN')],\n",
       " [('hey', 'NN')]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize([\"This is, my first dinner at Barcelona. Having a 7 blast!\", \"Hey you!\"],\n",
    "         POS=True,\n",
    "         stem=False,\n",
    "         remove_stopwords=True,\n",
    "         lower=True,\n",
    "         remove_nums=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training word2vec with dimensions= vectorsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drug', 'other', 'than', 'those', 'list', 'here', 'may', 'also', 'interact', 'with', 'glimepirid', 'or', 'affect', 'your', 'condit']\n",
      "8492\n",
      "latest loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "vector_size = 20\n",
    "window = 5\n",
    "pos = False\n",
    "stem = True\n",
    "remove_stopwords=False\n",
    "remove_punctuation=True\n",
    "lower=False\n",
    "remove_nums=False\n",
    "\n",
    "tokenized_sentences_pos = tokenize(sentences,\n",
    "                                   POS=pos,\n",
    "                                   stem=stem,\n",
    "                                   remove_stopwords=remove_stopwords,\n",
    "                                   remove_punctuation=remove_punctuation,\n",
    "                                   lower=lower,\n",
    "                                   remove_nums=remove_nums)\n",
    "print(tokenized_sentences_pos[0])\n",
    "print(len(tokenized_sentences_pos))  \n",
    "\n",
    "model = Word2Vec(tokenized_sentences_pos, size=vector_size, window=window, min_count=1, workers=cpu_count(), compute_loss=True)\n",
    "model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "print ('latest loss:', model.get_latest_training_loss())\n",
    "\n",
    "# save embeddings and delete model\n",
    "model.save(\"../word_vectors_stem_20\")\n",
    "#model = Word2Vec.load('../word_vectors')\n",
    "word_vectors = model.wv\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.40566888 -1.4040706  -1.3846399   0.33847553  0.12695037 -1.5943942\n",
      "  0.11700446 -1.3154646   0.62843055 -0.42977703 -0.9497059  -0.277214\n",
      "  0.6509919   0.41735038  1.3763509   0.9191987  -0.77937883  0.39092135\n",
      "  1.486061    1.0959866 ]\n"
     ]
    }
   ],
   "source": [
    "#print(word_vectors[\"conduction_NN\"]) \n",
    "#print(word_vectors[\"conduction\"]) \n",
    "#print(word_vectors[\"conduct_NN\"]) #stemmed + pos\n",
    "print(word_vectors[\"conduct\"])\n",
    "#print(word_vectors['105_CD'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_size = 20\n",
    "window = 5\n",
    "pos = False\n",
    "stem = True\n",
    "remove_stopwords=False\n",
    "remove_punctuation=True\n",
    "lower=False\n",
    "remove_nums=False\n",
    "\n",
    "\n",
    "def load_data(df, pos=True, stem=True, remove_stopwords=True, remove_punctuation=True, lower=False, remove_nums=True):    \n",
    "    sentences = [row['sentenceText'] for index, row in df.iterrows()]\n",
    "    sentenceIDs = [row['sentenceID'] for index, row in df.iterrows()]\n",
    "    tokenized_sentences_pos = tokenize(sentences, POS=pos, stem=stem,\n",
    "                                      remove_stopwords=remove_stopwords,\n",
    "                                      remove_punctuation=remove_punctuation,\n",
    "                                      lower=lower, remove_nums=False)\n",
    "    print(len(sentences), len(sentenceIDs), len(tokenized_sentences_pos))\n",
    "    \n",
    "    # compute size of dataset \n",
    "    count = 0\n",
    "    for sen, ID in (zip(tokenized_sentences_pos, sentenceIDs)):\n",
    "        count += len(label_dict[ID])\n",
    "        \n",
    "    # allocate memory\n",
    "    X = np.empty((count, vector_size))\n",
    "    Y = np.chararray((count))\n",
    "    \n",
    "    i = 0\n",
    "    for sen, ID in (zip(tokenized_sentences_pos, sentenceIDs)):\n",
    "        #print(sen, ID)\n",
    "        labels = label_dict[ID]\n",
    "        #print(labels)\n",
    "        for word, label in zip(sen, labels):\n",
    "                word_vector = word_vectors[word]\n",
    "                #print(word, label)\n",
    "                X[i] = word_vector\n",
    "                Y[i] = label\n",
    "                i += 1\n",
    "                #X = np.vstack((X, word_vector))\n",
    "                #Y = np.append(Y, label)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6832 6832 6832\n",
      "(148031, 20) (148031,)\n",
      "664 664 664\n",
      "(14896, 20) (14896,)\n",
      "1299 1299 1299\n",
      "(29839, 20) (29839,)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = load_data(sentences_df_train, pos, stem)\n",
    "X_test1, Y_test1 = load_data(sentences_df_test1, pos, stem)\n",
    "X_test2, Y_test2 = load_data(sentences_df_test2, pos, stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/dsaric/dev/HLT/XY/STEM_20/X_train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-60ac765c926f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'XY/STEM_20'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'X_train.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'X_test1.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'X_test2.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/dsaric/dev/HLT/XY/STEM_20/X_train.npy'"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(ROOT_DIR, 'XY/STEM_20')\n",
    "\n",
    "np.save(os.path.join(data_path, 'X_train.npy'), X_train)\n",
    "np.save(os.path.join(data_path, 'X_test1.npy'), X_test1)\n",
    "np.save(os.path.join(data_path, 'X_test2.npy'), X_test2)\n",
    "np.save(os.path.join(data_path, 'Y_train.npy'), Y_train)\n",
    "np.save(os.path.join(data_path, 'Y_test1.npy'), Y_test1)\n",
    "np.save(os.path.join(data_path, 'Y_test2.npy'), Y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (X_train.shape, Y_train.shape)\n",
    "print (X_test1.shape, Y_test1.shape)\n",
    "#print (X_test2.shape, Y_test2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode labels\n",
    "- Convert labels from B-I-O to $0, 1, 2$ for SVM\n",
    "- Convert labels from B-I-O to $[1 0 0, 0 1 0, 0 0 1]$ for ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode class values as integers = B-I-O -> 0-1-2\n",
    "encoder = LabelEncoder()\n",
    "encoded_Y = encoder.fit_transform(Y_train)\n",
    "Y_train = encoded_Y\n",
    "\n",
    "Y_test = encoder.fit_transform(Y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Creating the feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_capitalized(word):\n",
    "    if not word:\n",
    "        return 0\n",
    "    if word[0].isupper():\n",
    "        return 0.5\n",
    "    if word.isupper():\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def has_POS_NN(word):\n",
    "    if not word:\n",
    "        return 0\n",
    "    if pos_tag([word]) == 'NN':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "#pos_tag([\"cat\"])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "def has_numbers(word):\n",
    "    if isNumber(word):\n",
    "        return 0\n",
    "    else:\n",
    "        if hasNumbers(word):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0.5\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "surrounding = ['test', 'dosage', 'concentrations', 'induce', 'inhibit', 'treatment', 'coadministration',\n",
    "               'studies', 'supplements', 'intake', 'therapy', 'doses', 'use', 'given', 'administration']\n",
    "\n",
    "def possible_pos(sentence, word_position):\n",
    "    mx = min(len(sentence) - 1, word_position + 2)\n",
    "    mn = max(0, word_position - 2)\n",
    "    return range(mn, mx+1)\n",
    "\n",
    "def is_trigger(word, sentence, word_position):\n",
    "    if not word:\n",
    "        return 0\n",
    "    for word_pos in possible_pos(sentence, word_position):\n",
    "        if np.argmax([similar(sentence[word_pos], w) for w in surrounding])> 0.7:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vowels = [\"a\", \"e\", \"i\", \"o\", \"u\"]\n",
    "def has_more_consonants(word):\n",
    "    number_of_vowels = sum(word.count(c) for c in vowels)\n",
    "    number_of_consonants = len(word) - number_of_vowels\n",
    "    if number_of_consonants > number_of_vowels:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "surrounding = ['test', 'dosage', 'concentrations', 'induce', 'inhibit', 'treatment', 'coadministration',\n",
    "               'studies', 'supplements', 'intake', 'therapy', 'doses', 'use', 'given', 'administration', 'indicate']\n",
    "\n",
    "words = list()\n",
    "sentenceIDs = list()\n",
    "\n",
    "def prepare_words(df):\n",
    "    sentences_train1 = [row['sentenceText'] for index, row in df.iterrows()]\n",
    "    sentenceIDs_train = [row['sentenceID'] for index, row in df.iterrows()]\n",
    "    sentences_train = tokenize(sentences_train1, POS=False, stem=False, remove_stopwords=True, remove_nums=True)\n",
    "\n",
    "    for ID, sentence in zip(sentences_train1, sentences_train):\n",
    "        for word in sentence:\n",
    "            words.append(word)\n",
    "            sentenceIDs.append(ID)\n",
    "    \n",
    "    s = {'sentence':sentenceIDs, 'word':words}\n",
    "    prep = pd.DataFrame(data=s)\n",
    "    return prep\n",
    "\n",
    "train = prepare_words(sentences_df_train)\n",
    "train.info()\n",
    "print(len(sentences_df_train))\n",
    "\n",
    "test = prepare_words(sentences_df_test1)\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "vector_size = 25\n",
    "\n",
    "\n",
    "X_tr = np.array([]).reshape(0,vector_size)\n",
    "for index, row in train.iterrows():\n",
    "    word = row['word']\n",
    "    sentence = word_tokenize(row['sentence'])\n",
    "    word_position = sentence.index(word)\n",
    "    print(word, word_position, sentence[word_position])\n",
    "    vector = [is_capitalized(word), is_trigger(word, sentence, word_position), has_numbers(word), has_POS_NN(word), has_more_consonants(word)] # + add embedding\n",
    "    #print(vector)similar(\"Apple\",\"Appel\")\n",
    "    sentence = tokenize([row['sentence']])\n",
    "    print(sentence, word_position)\n",
    "    if word_position >=len(sentence[0]):\n",
    "        word_position = len(sentence[0]) - 1\n",
    "    vector.extend(word_vectors[sentence[0][word_position]].tolist())\n",
    "    print(vector)\n",
    "    vector = np.array(vector)\n",
    "    #print(embed, vector)\n",
    "    #vector = list(vector + embed.T)\n",
    "    print(len(vector), X_tr.shape)\n",
    "    vector = np.array(vector).reshape(1, -1)\n",
    "    print(vector.shape, X_tr.shape)\n",
    "    X_tr = np.vstack((X_tr, vector))\n",
    "    \n",
    "print(X_tr.shape)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ts = np.array([]).reshape(0,vector_size)\n",
    "for index, row in test.iterrows():\n",
    "    word = row['word']\n",
    "    sentence = word_tokenize(row['sentence'])\n",
    "    word_position = sentence.index(word)\n",
    "    print(word, word_position, sentence[word_position])\n",
    "    vector = [is_capitalized(word), is_trigger(word, sentence, word_position), has_numbers(word), has_POS_NN(word), has_more_consonants(word)] # + add embedding\n",
    "    #print(vector)similar(\"Apple\",\"Appel\")\n",
    "    sentence = tokenize([row['sentence']])\n",
    "    print(sentence, word_position)\n",
    "    if word_position >=len(sentence[0]):\n",
    "        word_position = len(sentence[0]) - 1\n",
    "    vector.extend(word_vectors[sentence[0][word_position]].tolist())\n",
    "    print(vector)\n",
    "    vector = np.array(vector)\n",
    "    #print(embed, vector)\n",
    "    #vector = list(vector + embed.T)\n",
    "    print(len(vector), X_ts.shape)\n",
    "    vector = np.array(vector).reshape(1, -1)\n",
    "    print(vector.shape, X_ts.shape)\n",
    "    X_ts = np.vstack((X_ts, vector))\n",
    "    \n",
    "print(X_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# one-vs-all classifier\n",
    "model = svm.SVC(kernel='rbf', \n",
    "                C=1.0,\n",
    "                class_weight=None,\n",
    "                gamma='auto',\n",
    "                #penalty='l2',\n",
    "                #loss='squared_hinge',\n",
    "                tol=0.001, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_tr, Y_train)\n",
    "predictions = model.predict(X_ts, Y_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X_tr.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X_ts.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
