{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# to make nbs importable\n",
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# custom\n",
    "from analize_text import get_sentenceID\n",
    "from paths import *\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading sentences from structured data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataframes of sentences and entities\n",
    "\n",
    "# TRAIN SET\n",
    "sentences_df_train = pd.read_csv(SENTENCE_PATH_train)\n",
    "entities_df_train = pd.read_csv(ENTITY_PATH_train)\n",
    "\n",
    "#TEST SET\n",
    "sentences_df_test1 = pd.read_csv(SENTENCE_PATH_test1)\n",
    "entities_df_test1 = pd.read_csv(ENTITY_PATH_test1)\n",
    "\n",
    "#TEST2 SET\n",
    "sentences_df_test2 = pd.read_csv(SENTENCE_PATH_test2)\n",
    "entities_df_test2 = pd.read_csv(ENTITY_PATH_test2)\n",
    "\n",
    "print(len(sentences_df_train), len(sentences_df_test1), len(sentences_df_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenating training and test data for the word2vec training!\n",
    "sentences_df = pd.concat([sentences_df_train,\n",
    "                          sentences_df_test1,\n",
    "                          sentences_df_test2]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "entities_df = pd.concat([entities_df_train,\n",
    "                         entities_df_test1,\n",
    "                         entities_df_test2]).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sentences dataframe')\n",
    "sentences_df_train.head()\n",
    "#sentences_df_test2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entities dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Entities dataframe')\n",
    "entities_df_train.head()\n",
    "#entities_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load label dictionary {sentenceID: [ 'B', 'I', ..., 'O'] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_dict_path = os.path.join(ROOT_DIR, 'Train', 'bio_labels')\n",
    "label_dict = np.load(label_dict_path + '.npy').item()\n",
    "\n",
    "sentenceIDs = label_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [row['sentenceText'] for index, row in sentences_df.iterrows()]\n",
    "# remove duplicates from sentence list (sentences with e.g. 2 entities appeared twice)\n",
    "sentences = list(set(sentences))\n",
    "#print(sentences)\n",
    "print(len(sentences))\n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming + POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS needs to be extracted before stemming appended afterwards\n",
    "# TODO: try which one performs better\n",
    "stemmer = EnglishStemmer()\n",
    "s = ['interaction', 'between', 'cimetidine', 'and', 'warfarin', 'could', 'be', 'dangerous']\n",
    "\n",
    "print ('original: ', s)\n",
    "print('stemmed:  ', [stemmer.stem(w) for w in s])\n",
    "print('Conclusion: It stemms the drug names too.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending POS tags\n",
    "- tokens will have the form: stemmed-word_POS if argument stem=True\n",
    "- tokens will have the form: word_POS         if argument stem=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_with_POS(sentences, stem=True):\n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    tokenized_pos = pos_tag_sents(tokenized_sentences, tagset=None)\n",
    "    if stem:\n",
    "        tokenized_pos = [ [stemmer.stem(w) + '_' + pos for w, pos in s ] for s in tokenized_pos]\n",
    "    else:\n",
    "        tokenized_pos = [ [w + '_' + pos for w, pos in s ] for s in tokenized_pos]\n",
    "    return tokenized_pos  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training word2vec with dimensions= vectorsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 20\n",
    "\n",
    "tokenized_sentences_pos = tokenize_with_POS(sentences)\n",
    "print(tokenized_sentences_pos[0])\n",
    "print(len(tokenized_sentences_pos))  \n",
    "\n",
    "model = Word2Vec(tokenized_sentences_pos, size=vector_size, window=5, min_count=1, workers=cpu_count(), compute_loss=True)\n",
    "model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "print ('latest loss:', model.get_latest_training_loss())\n",
    "\n",
    "# save embeddings and delete model\n",
    "model.save(\"../word_vectors\")\n",
    "#model = Word2Vec.load('../word_vectors')\n",
    "word_vectors = model.wv\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(word_vectors[\"conduction_NN\"]) \n",
    "print(word_vectors[\"conduct_NN\"]) #stemmed\n",
    "print(word_vectors['105_CD'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(df):\n",
    "    X = np.array([]).reshape(0,vector_size)\n",
    "    Y = np.array([])\n",
    "    sentences = [row['sentenceText'] for index, row in df.iterrows()]\n",
    "    sentenceIDs = [row['sentenceID'] for index, row in df.iterrows()]\n",
    "    tokenized_sentences_pos = tokenize_with_POS(sentences)\n",
    "    print(len(sentences), len(sentenceIDs), len(tokenized_sentences_pos))\n",
    "    for sen, ID in (zip(tokenized_sentences_pos, sentenceIDs)):\n",
    "        #print(sen, ID)\n",
    "        labels = label_dict[ID]\n",
    "        #print(labels)\n",
    "        for word, label in zip(sen, labels):\n",
    "                word_vector = word_vectors[word]\n",
    "                #print(word, label)\n",
    "                X = np.vstack((X, word_vector))\n",
    "                Y = np.append(Y, label)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create X_train, Y_train\n",
    "X_train = np.array([]).reshape(0,vector_size)\n",
    "Y_train = np.array([])\n",
    "\n",
    "X_test1 = np.array([]).reshape(0,vector_size)\n",
    "Y_test1 = np.array([])\n",
    "\n",
    "X_test2 = np.array([]).reshape(0,vector_size)\n",
    "Y_test2 = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = load_data(sentences_df_train)\n",
    "X_test1, Y_test1 = load_data(sentences_df_test1)\n",
    "X_test2, Y_test2 = load_data(sentences_df_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(ROOT_DIR, 'XY')\n",
    "\n",
    "np.save(os.path.join(data_path, 'X_train.npy'), X_train)\n",
    "np.save(os.path.join(data_path, 'X_test1.npy'), X_test1)\n",
    "np.save(os.path.join(data_path, 'X_test2.npy'), X_test2)\n",
    "np.save(os.path.join(data_path, 'Y_train.npy'), Y_train)\n",
    "np.save(os.path.join(data_path, 'Y_test1.npy'), Y_test1)\n",
    "np.save(os.path.join(data_path, 'Y_test2.npy'), Y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X_train.shape, Y_train.shape)\n",
    "print (X_test1.shape, Y_test1.shape)\n",
    "print (X_test2.shape, Y_test2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode labels\n",
    "- Convert labels from B-I-O to $0, 1, 2$ for SVM\n",
    "- Convert labels from B-I-O to $[1 0 0, 0 1 0, 0 0 1]$ for ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode class values as integers = B-I-O -> 0-1-2\n",
    "encoder = LabelEncoder()\n",
    "encoded_Y = encoder.fit_transform(Y_train)\n",
    "Y_train = encoded_Y\n",
    "# convert integers to one-hot encoding\n",
    "Y_train_one_hot = np_utils.to_categorical(encoded_Y) # SVM does not need one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
