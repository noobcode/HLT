{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# custom\n",
    "from analize_text import get_sentenceID\n",
    "from paths import *\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# scikit learn\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading sentences from structured data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read dataframes of sentences and entities\n",
    "\n",
    "# TRAIN SET\n",
    "sentences_df_train = pd.read_csv(SENTENCE_PATH_train)\n",
    "entities_df_train = pd.read_csv(ENTITY_PATH_train)\n",
    "\n",
    "#TEST SET\n",
    "sentences_df_test1 = pd.read_csv(SENTENCE_PATH_test1)\n",
    "entities_df_test1 = pd.read_csv(ENTITY_PATH_test1)\n",
    "\n",
    "#TEST2 SET\n",
    "sentences_df_test2 = pd.read_csv(SENTENCE_PATH_test2)\n",
    "entities_df_test2 = pd.read_csv(ENTITY_PATH_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Concatenating training and test data for the word2vec training!\n",
    "sentences_df = pd.concat([sentences_df_train,\n",
    "                          sentences_df_test1,\n",
    "                          sentences_df_test2]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "entities_df = pd.concat([entities_df_train,\n",
    "                         entities_df_test1,\n",
    "                         entities_df_test2]).drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences dataframe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentenceID</th>\n",
       "      <th>sentenceText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DDI-DrugBank.d281.s0</td>\n",
       "      <td>Probenecid may decrease renal tubular secretio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DDI-DrugBank.d281.s1</td>\n",
       "      <td>Drug/Laboratory Test Interactions A false posi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DDI-DrugBank.d281.s2</td>\n",
       "      <td>Positive direct and indirect antiglobulin (Coo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DDI-DrugBank.d281.s3</td>\n",
       "      <td>these may also occur in neonates whose mothers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DDI-DrugBank.d384.s0</td>\n",
       "      <td>Interactions for vitamin D analogues (Vitamin ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             sentenceID                                       sentenceText\n",
       "0  DDI-DrugBank.d281.s0  Probenecid may decrease renal tubular secretio...\n",
       "1  DDI-DrugBank.d281.s1  Drug/Laboratory Test Interactions A false posi...\n",
       "2  DDI-DrugBank.d281.s2  Positive direct and indirect antiglobulin (Coo...\n",
       "3  DDI-DrugBank.d281.s3  these may also occur in neonates whose mothers...\n",
       "4  DDI-DrugBank.d384.s0  Interactions for vitamin D analogues (Vitamin ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Sentences dataframe')\n",
    "sentences_df_train.head()\n",
    "#sentences_df_test2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entities dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities dataframe\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entityID</th>\n",
       "      <th>name</th>\n",
       "      <th>position</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DDI-DrugBank.d281.s0.e0</td>\n",
       "      <td>Probenecid</td>\n",
       "      <td>0-9</td>\n",
       "      <td>drug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DDI-DrugBank.d281.s0.e1</td>\n",
       "      <td>cephalosporins</td>\n",
       "      <td>51-64</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DDI-DrugBank.d281.s0.e2</td>\n",
       "      <td>cephalosporin</td>\n",
       "      <td>132-144</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DDI-DrugBank.d281.s3.e0</td>\n",
       "      <td>cephalosporins</td>\n",
       "      <td>56-69</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DDI-DrugBank.d384.s0.e0</td>\n",
       "      <td>vitamin D analogues</td>\n",
       "      <td>17-35</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  entityID                 name position   type\n",
       "0  DDI-DrugBank.d281.s0.e0           Probenecid      0-9   drug\n",
       "1  DDI-DrugBank.d281.s0.e1       cephalosporins    51-64  group\n",
       "2  DDI-DrugBank.d281.s0.e2        cephalosporin  132-144  group\n",
       "3  DDI-DrugBank.d281.s3.e0       cephalosporins    56-69  group\n",
       "4  DDI-DrugBank.d384.s0.e0  vitamin D analogues    17-35  group"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Entities dataframe')\n",
    "entities_df_train.head()\n",
    "#entities_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load label dictionary {sentenceID: [ 'B', 'I', ..., 'O'] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_dict_path = os.path.join(ROOT_DIR, 'Train', 'bio_labels')\n",
    "label_dict = np.load(label_dict_path + '.npy').item()\n",
    "\n",
    "sentenceIDs = label_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8795\n",
      "8492\n"
     ]
    }
   ],
   "source": [
    "sentences = [row['sentenceText'] for index, row in sentences_df.iterrows()]\n",
    "# remove duplicates from sentence list (sentences with e.g. 2 entities appeared twice)\n",
    "sentences = list(set(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8492"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "len(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma (todo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed version:\n",
      "\n",
      "['interact', 'between', 'cimetidin', 'and', 'warfarin', 'could', 'be', 'danger']\n",
      "\n",
      "original pos tags:\n",
      "\n",
      "[('interaction', 'NN'), ('between', 'IN'), ('cimetidine', 'NN'), ('and', 'CC'), ('warfarin', 'NN'), ('could', 'MD'), ('be', 'VB'), ('dangerous', 'JJ')]\n",
      "\n",
      "stemmed pos tags:\n",
      "\n",
      "[('interact', 'NN'), ('between', 'IN'), ('cimetidin', 'NN'), ('and', 'CC'), ('warfarin', 'NN'), ('could', 'MD'), ('be', 'VB'), ('danger', 'JJR')]\n"
     ]
    }
   ],
   "source": [
    "### EXAMPLE STEM + POS ####\n",
    "# POS could differ slightly when applied to the stemmed version or not\n",
    "# TODO: try which one performs better\n",
    "stemmer = EnglishStemmer()\n",
    "s = ['interaction', 'between', 'cimetidine', 'and', 'warfarin', 'could', 'be', 'dangerous']\n",
    "\n",
    "print ('stemmed version:\\n')\n",
    "stemmed_s = [stemmer.stem(w) for w in s]\n",
    "print (stemmed_s)\n",
    "\n",
    "print ('\\noriginal pos tags:\\n')\n",
    "print(pos_tag(s))\n",
    "\n",
    "print ('\\nstemmed pos tags:\\n')\n",
    "print(pos_tag(stemmed_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fourteen_NNP',\n",
       " 'days_NNS',\n",
       " 'later_RB',\n",
       " ',_,',\n",
       " 'all_DT',\n",
       " 'animals_NNS',\n",
       " 'were_VBD',\n",
       " 'challenged_VBN',\n",
       " 'with_IN',\n",
       " 'a_DT',\n",
       " 'single_JJ',\n",
       " 'hypnotic_JJ',\n",
       " 'dose_NN',\n",
       " 'of_IN',\n",
       " 'ethanol_NN',\n",
       " '(_(',\n",
       " '3.5_CD',\n",
       " 'g/kg_NN',\n",
       " 'IP_NNP',\n",
       " ')_)',\n",
       " '._.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences_pos = pos_tag_sents(tokenized_sentences, tagset=None) # tagset = None, 'universal', 'wsj', 'brown'\n",
    "\n",
    "# concatenate the part of speach to each word (e.g. cat_NN)\n",
    "tokenized_sentences_pos = [ [w + '_' + pos for w, pos in s ] for s in tokenized_sentences_pos]\n",
    "tokenized_sentences_pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "vector_size = 20\n",
    "model = Word2Vec(tokenized_sentences_pos, size=vector_size, window=5, min_count=1, workers=cpu_count(), compute_loss=True)\n",
    "model.train(sentences, total_examples=len(sentences), epochs=10)\n",
    "print ('latest loss:', model.get_latest_training_loss())\n",
    "\n",
    "# save embeddings and delete model\n",
    "model.save(\"../word_vectors\")\n",
    "#model = Word2Vec.load('../word_vectors')\n",
    "word_vectors = model.wv\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.25848603  0.08273632 -0.08718783  0.27140656  0.27854267 -0.02318844\n",
      " -0.25571734 -0.17348506 -0.20984288 -0.2621599  -0.22950882 -0.3967506\n",
      "  0.35958096  0.24631514  0.01595954  0.10744167  0.12884106 -0.6890047\n",
      "  0.25285932 -0.4622117 ]\n",
      "[ 0.00748834  0.03906478  0.0067243   0.01321432  0.02106611  0.02975854\n",
      " -0.03512239  0.00209284 -0.00719997 -0.0222114  -0.01517265 -0.00194615\n",
      "  0.01409792  0.03328705  0.01150845  0.01233158 -0.01857619 -0.04219748\n",
      "  0.01378438 -0.03474882]\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors[\"conduction_NN\"])\n",
    "print(word_vectors['105_CD'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(df):\n",
    "    X = np.array([]).reshape(0,vector_size)\n",
    "    words = np.array([])\n",
    "    Y = np.array([])\n",
    "    for sentenceID, labels in label_dict.items():\n",
    "        if df[df.sentenceID == sentenceID].empty:\n",
    "            #print('empty')\n",
    "            continue\n",
    "        else: \n",
    "            sentence = df[df.sentenceID == sentenceID]['sentenceText'].values[0] \n",
    "        tok_sentence = word_tokenize(sentence)\n",
    "        tok_sentence_pos = [ word + '_' + pos for word, pos in pos_tag(tok_sentence, tagset=None)]\n",
    "\n",
    "        for word, label in zip(tok_sentence_pos, labels):\n",
    "            word_vector = word_vectors[word]\n",
    "            X = np.vstack((X, word_vector))\n",
    "            Y = np.append(Y, label)       \n",
    "            words = np.append(words, word)\n",
    "            #print(word)\n",
    "    return X, Y, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create X_train, Y_train\n",
    "X_train = np.array([]).reshape(0,vector_size)\n",
    "Y_train = np.array([])\n",
    "words_train = np.array([])\n",
    "\n",
    "X_test1 = np.array([]).reshape(0,vector_size)\n",
    "Y_test1 = np.array([])\n",
    "words_test1 = np.array([])\n",
    "\n",
    "#X_test2 = np.array([]).reshape(0,vector_size)\n",
    "#Y_test2 = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, words_train = load_data(sentences_df_train)\n",
    "X_test1, Y_test1, words_test1 = load_data(sentences_df_test1)\n",
    "#X_test2, Y_test2, words_test2 = load_data(sentences_df_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (X_train.shape, Y_train.shape)\n",
    "print (X_test1.shape, Y_test1.shape)\n",
    "#print (X_test2.shape, Y_test2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode labels\n",
    "- Convert labels from B-I-O to $0, 1, 2$ for SVM\n",
    "- Convert labels from B-I-O to $[1 0 0, 0 1 0, 0 0 1]$ for ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode class values as integers = B-I-O -> 0-1-2\n",
    "encoder = LabelEncoder()\n",
    "encoded_Y = encoder.fit_transform(Y_train)\n",
    "Y_train = encoded_Y\n",
    "# convert integers to one-hot encoding\n",
    "Y_train_one_hot = np_utils.to_categorical(encoded_Y) # SVM does not need one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
