{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# to make nbs importable\n",
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "#from nbs_import import NotebookLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# custom\n",
    "from analize_text import get_sentenceID\n",
    "from paths import *\n",
    "\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# scikit learn\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# keras\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from feature_transformer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset (embeddings_POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(ROOT_DIR, 'XY', 'stemPOS')\n",
    "X_test2 = np.load(os.path.join(data_path, 'X_test2.npy'))\n",
    "X_test1 = np.load(os.path.join(data_path, 'X_test1.npy'))\n",
    "X_train = np.load(os.path.join(data_path, 'X_train.npy'))\n",
    "\n",
    "Y_test2 = np.load(os.path.join(data_path, 'Y_test2.npy'))\n",
    "Y_test1 = np.load(os.path.join(data_path, 'Y_test1.npy'))\n",
    "Y_train = np.load(os.path.join(data_path, 'Y_train.npy'))\n",
    "print(X_test1.shape, Y_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2vmodel = Word2Vec.load('../word_vectors')\n",
    "word_vectors = w2vmodel.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode labels\n",
    "- Convert labels from B-I-O to $0, 1, 2$ for SVM\n",
    "- Convert labels from B-I-O to $[1 0 0, 0 1 0, 0 0 1]$ for ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode class values as integers = B-I-O -> 0-1-2\n",
    "encoder = LabelEncoder()\n",
    "encoded_Y = encoder.fit_transform(Y_train)\n",
    "Y_train = encoded_Y\n",
    "# convert integers to one-hot encoding\n",
    "Y_train_one_hot = np_utils.to_categorical(encoded_Y) # SVM does not need one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating feature transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Embedder(FeatureTransformer):\n",
    "  # returns embedding for each word\n",
    "  def transform(self, X, y=None, **fit_params):\n",
    "    return np.array([word_vectors[x] for x in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = Embedder()\n",
    "e = emb.transform(['conduction_NN', words_train[7]])\n",
    "print(e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Capitalizer(FeatureTransformer):\n",
    "    # return 1 if the word is capitalized\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return np.array([1 if x[0].isupper() else 0 for x in X]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = Capitalizer()\n",
    "c = cap.transform(['conduction_NN', words_train[7]])\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "clf = LinearSVC()\n",
    "\n",
    "pipe = FeatureUnion([\n",
    "    ('emb', Embedder()),\n",
    "    ('cap', Capitalizer()),\n",
    "])\n",
    "\n",
    "print(words_train.reshape(-1, 1).shape)\n",
    "data = pipe.transform(words_train.reshape(-1, 1))\n",
    "print(data.shape)\n",
    "#clf.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "word_tokens = word_tokenize(example_sent)\n",
    " \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    " \n",
    "filtered_sentence = []\n",
    " \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    " \n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PunctuationRemoval(FeatureTransformer):\n",
    "    def transform():\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "experiment with:\n",
    "- original data\n",
    "- MinMaxScaler\n",
    "- Standardizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#min_max_scaler = MinMaxScaler()\n",
    "#X_train = min_max_scaler.fit_transform(X_train)\n",
    "\n",
    "#standard_scaler = StandardScaler()\n",
    "#X_train = standard_scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split train / validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split train validation (SVM)\n",
    "train_perc = 0.9\n",
    "train_size = int(len(X_train) * train_perc)\n",
    "\n",
    "X_tr, X_vl = X_train[:train_size,:], X_train[train_size:,:]\n",
    "Y_tr, Y_vl = Y_train[:train_size], Y_train[train_size:]\n",
    "\n",
    "print (\"X train/validation shapes:\", X_tr.shape, X_vl.shape)\n",
    "print (\"Y train/validation shapes (svm):\", Y_tr.shape, Y_vl.shape)\n",
    "\n",
    "# split train validatioin (NN)\n",
    "Y_tr_nn, Y_vl_nn = Y_train_one_hot[:train_size], Y_train_one_hot[train_size:]\n",
    "print (\"Y train/validation shapes: (ann): \", Y_tr_nn.shape, Y_vl_nn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# one-vs-all classifier\n",
    "model = svm.SVC(kernel='rbf', \n",
    "                C=1.0,\n",
    "                class_weight=None,\n",
    "                gamma='auto',\n",
    "                #penalty='l2',\n",
    "                #loss='squared_hinge',\n",
    "                tol=0.001, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_tr, Y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute metrics\n",
    "- accuracy\n",
    "- f1-score (micro, macro, weighted)\n",
    "- precision (micro, macro, weighted)\n",
    "- recall (micro, macro, weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_vl)\n",
    "\n",
    "# micro:\n",
    "# computes metrics globally - considering total number of TP, FP, FN\n",
    "f1_micro = f1_score(Y_vl, predictions, average='micro')\n",
    "recall_micro = recall_score(Y_vl, predictions, average='micro')\n",
    "precision_micro = precision_score(Y_vl, predictions, average='micro')\n",
    "\n",
    "# macro:\n",
    "# compute metrics for each label, then unweighted average them - does not take class imbalance into account\n",
    "f1_macro = f1_score(Y_vl, predictions, average='macro')\n",
    "recall_macro = recall_score(Y_vl, predictions, average='macro')\n",
    "precision_macro = precision_score(Y_vl, predictions, average='macro')\n",
    "\n",
    "# weighted:\n",
    "# as macro, but taking into account true class frequencies for each label\n",
    "f1_weighted = f1_score(Y_vl, predictions, average='weighted')\n",
    "recall_weighted = recall_score(Y_vl, predictions, average='weighted')\n",
    "precision_weighted = precision_score(Y_vl, predictions, average='weighted')\n",
    "\n",
    "accuracy = accuracy_score(Y_vl, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print ('SVM validation accuracy:', round(accuracy, 4))\n",
    "print ('-'*20)\n",
    "print ('SVM validation f1-score (micro):', round(f1_micro, 4))\n",
    "print ('SVM validation precision (micro):', round(precision_micro, 4))\n",
    "print ('SVM validation recall (micro):', round(recall_micro, 4))\n",
    "print ('-'*20)\n",
    "print ('SVM validation f1-score (macro):', round(f1_macro, 4))\n",
    "print ('SVM validation precision (macro):', round(precision_macro, 4))\n",
    "print ('SVM validation recall (macro):', round(recall_macro, 4))\n",
    "print ('-'*20)\n",
    "print ('SVM validation f1-score (weighted):', round(f1_weighted, 4))\n",
    "print ('SVM validation precision (weighted):', round(precision_weighted, 4))\n",
    "print ('SVM validation recall (weighted):', round(recall_weighted, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n",
    "IdSentence|startOffset-endOffset|text|null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
