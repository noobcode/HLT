{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# to make nbs importable\n",
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "#from nbs_import import NotebookLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# custom\n",
    "from analize_text import get_sentenceID\n",
    "from paths import *\n",
    "\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# scikit learn\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "# keras\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from feature_transformer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read dataframes of sentences and entities\n",
    "\n",
    "# TRAIN SET\n",
    "sentences_df_train = pd.read_csv(SENTENCE_PATH_train)\n",
    "entities_df_train = pd.read_csv(ENTITY_PATH_train)\n",
    "\n",
    "#TEST SET\n",
    "sentences_df_test1 = pd.read_csv(SENTENCE_PATH_test1)\n",
    "entities_df_test1 = pd.read_csv(ENTITY_PATH_test1)\n",
    "\n",
    "#TEST2 SET\n",
    "sentences_df_test2 = pd.read_csv(SENTENCE_PATH_test2)\n",
    "entities_df_test2 = pd.read_csv(ENTITY_PATH_test2)\n",
    "\n",
    "print(len(sentences_df_train), len(sentences_df_test1), len(sentences_df_test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset (embeddings_POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148031, 20) (148031,)\n",
      "(14896, 20) (14896,)\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(ROOT_DIR, 'XY', 'LEMMA_20')\n",
    "X_test2 = np.load(os.path.join(data_path, 'X_test2.npy'))\n",
    "X_test1 = np.load(os.path.join(data_path, 'X_test1.npy'))\n",
    "X_train = np.load(os.path.join(data_path, 'X_train.npy'))\n",
    "\n",
    "Y_test2 = np.load(os.path.join(data_path, 'Y_test2.npy'))\n",
    "Y_test1 = np.load(os.path.join(data_path, 'Y_test1.npy'))\n",
    "Y_train = np.load(os.path.join(data_path, 'Y_train.npy'))\n",
    "\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test1.shape, Y_test1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2vmodel = Word2Vec.load('../word_vectors_lemma_20')\n",
    "word_vectors = w2vmodel.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode labels\n",
    "- Convert labels from B-I-O to $0, 1, 2$ for SVM\n",
    "- Convert labels from B-I-O to $[1 0 0, 0 1 0, 0 0 1]$ for ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode class values as integers = B-I-O -> 0-1-2\n",
    "encoder = LabelEncoder()\n",
    "encoded_Y = encoder.fit_transform(Y_train)\n",
    "Y_train = encoded_Y\n",
    "# convert integers to one-hot encoding\n",
    "Y_train_one_hot = np_utils.to_categorical(encoded_Y) # SVM does not need one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "experiment with:\n",
    "- original data\n",
    "- MinMaxScaler\n",
    "- Standardizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#min_max_scaler = MinMaxScaler()\n",
    "#X_train = min_max_scaler.fit_transform(X_train)\n",
    "\n",
    "#standard_scaler = StandardScaler()\n",
    "#X_train = standard_scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train / validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train/validation shapes: (133227, 20) (14804, 20)\n",
      "Y train/validation shapes:  (133227, 3) (14804, 3)\n"
     ]
    }
   ],
   "source": [
    "train_perc = 0.9\n",
    "train_size = int(len(X_train) * train_perc)\n",
    "\n",
    "# split train validatioin (NN)\n",
    "X_tr, X_vl = X_train[:train_size,:], X_train[train_size:,:]\n",
    "Y_tr_nn, Y_vl_nn = Y_train_one_hot[:train_size], Y_train_one_hot[train_size:]\n",
    "\n",
    "print (\"X train/validation shapes:\", X_tr.shape, X_vl.shape)\n",
    "print (\"Y train/validation shapes: \", Y_tr_nn.shape, Y_vl_nn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from sklearn import preprocessing\n",
    "from keras.optimizers import *\n",
    "from keras.initializers import *\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 512)               10752     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 142,851\n",
      "Trainable params: 142,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_inputs = X_train.shape[1] # size of a vector\n",
    "num_outputs = 3 # b-i-o tags\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=512, input_shape=(num_inputs,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dense(units=128, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(units=num_outputs, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training callbacks\n",
    "- F1-score\n",
    "- Precision\n",
    "- Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class Metrics(Callback):\n",
    "\n",
    "    def __init__(self, x_val, y_val):\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s_micro = []\n",
    "        self.val_recalls_micro = []\n",
    "        self.val_precisions_micro = []\n",
    "        self.val_f1s_macro = []\n",
    "        self.val_recalls_macro = []\n",
    "        self.val_precisions_macro = []\n",
    "        self.val_f1s_weighted = []\n",
    "        self.val_recalls_weighted = []\n",
    "        self.val_precisions_weighted = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = np.argmax((np.asarray(self.model.predict(self.x_val))), axis=1)\n",
    "        val_targ = np.argmax(self.y_val, axis=1)\n",
    "        # micro\n",
    "        f1_micro = f1_score(val_targ, val_predict, average='micro')\n",
    "        recall_micro = recall_score(val_targ, val_predict, average='micro')\n",
    "        precision_micro = precision_score(val_targ, val_predict, average='micro')\n",
    "        # macro\n",
    "        f1_macro = f1_score(val_targ, val_predict, average='macro')\n",
    "        recall_macro = recall_score(val_targ, val_predict, average='macro')\n",
    "        precision_macro = precision_score(val_targ, val_predict, average='macro')\n",
    "        # weighted\n",
    "        f1_weighted = f1_score(val_targ, val_predict, average='weighted')\n",
    "        recall_weighted = recall_score(val_targ, val_predict, average='weighted')\n",
    "        precision_weighted = precision_score(val_targ, val_predict, average='weighted')\n",
    "        \n",
    "        # append metrics to access them later\n",
    "        # micro\n",
    "        self.val_f1s_micro.append(f1_micro)\n",
    "        self.val_recalls_micro.append(recall_micro)\n",
    "        self.val_precisions_micro.append(precision_micro)\n",
    "        # macro\n",
    "        self.val_f1s_macro.append(f1_macro)\n",
    "        self.val_recalls_macro.append(recall_macro)\n",
    "        self.val_precisions_macro.append(precision_macro)\n",
    "        # weighted\n",
    "        self.val_f1s_weighted.append(f1_weighted)\n",
    "        self.val_recalls_weighted.append(recall_weighted)\n",
    "        self.val_precisions_weighted.append(precision_weighted)\n",
    "        \n",
    "        print (\" — val_f1_micro: %.4f — val_precision_micro: %.4f — val_recall_micro: %.4f\" % (f1_micro, precision_micro, recall_micro))\n",
    "        print (\" — val_f1_macro: %.4f — val_precision_macro: %.4f — val_recall_macro: %.4f\" % (f1_macro, precision_macro, recall_macro))\n",
    "        print (\" — val_f1_weighted: %.4f — val_precision_weighted: %.4f — val_recall_weighted: %.4f\" % (f1_micro, precision_micro, recall_micro))\n",
    "        return\n",
    "\n",
    "    \n",
    "metrics_callback = Metrics(x_val=X_vl, y_val=Y_vl_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 133227 samples, validate on 14804 samples\n",
      "Epoch 1/30\n",
      "133227/133227 [==============================] - 26s 195us/step - loss: 0.2520 - acc: 0.8968 - val_loss: 0.1756 - val_acc: 0.9483\n",
      " — val_f1_micro: 0.9483 — val_precision_micro: 0.9483 — val_recall_micro: 0.9483\n",
      " — val_f1_macro: 0.4393 — val_precision_macro: 0.5585 — val_recall_macro: 0.4074\n",
      " — val_f1_weighted: 0.9483 — val_precision_weighted: 0.9483 — val_recall_weighted: 0.9483\n",
      "Epoch 2/30\n",
      "   896/133227 [..............................] - ETA: 24s - loss: 0.2270 - acc: 0.8940"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133227/133227 [==============================] - 25s 185us/step - loss: 0.2264 - acc: 0.9066 - val_loss: 0.2023 - val_acc: 0.9420\n",
      " — val_f1_micro: 0.9420 — val_precision_micro: 0.9420 — val_recall_micro: 0.9420\n",
      " — val_f1_macro: 0.4599 — val_precision_macro: 0.5120 — val_recall_macro: 0.4351\n",
      " — val_f1_weighted: 0.9420 — val_precision_weighted: 0.9420 — val_recall_weighted: 0.9420\n",
      "Epoch 3/30\n",
      "133227/133227 [==============================] - 25s 188us/step - loss: 0.2224 - acc: 0.9087 - val_loss: 0.1879 - val_acc: 0.9424\n",
      " — val_f1_micro: 0.9424 — val_precision_micro: 0.9424 — val_recall_micro: 0.9424\n",
      " — val_f1_macro: 0.4577 — val_precision_macro: 0.6592 — val_recall_macro: 0.4328\n",
      " — val_f1_weighted: 0.9424 — val_precision_weighted: 0.9424 — val_recall_weighted: 0.9424\n",
      "Epoch 4/30\n",
      "133227/133227 [==============================] - 25s 184us/step - loss: 0.2184 - acc: 0.9103 - val_loss: 0.1844 - val_acc: 0.9458\n",
      " — val_f1_micro: 0.9458 — val_precision_micro: 0.9458 — val_recall_micro: 0.9458\n",
      " — val_f1_macro: 0.4893 — val_precision_macro: 0.5835 — val_recall_macro: 0.4635\n",
      " — val_f1_weighted: 0.9458 — val_precision_weighted: 0.9458 — val_recall_weighted: 0.9458\n",
      "Epoch 5/30\n",
      "133227/133227 [==============================] - 27s 199us/step - loss: 0.2154 - acc: 0.9119 - val_loss: 0.1795 - val_acc: 0.9474\n",
      " — val_f1_micro: 0.9474 — val_precision_micro: 0.9474 — val_recall_micro: 0.9474\n",
      " — val_f1_macro: 0.4873 — val_precision_macro: 0.6166 — val_recall_macro: 0.4562\n",
      " — val_f1_weighted: 0.9474 — val_precision_weighted: 0.9474 — val_recall_weighted: 0.9474\n",
      "Epoch 6/30\n",
      "133227/133227 [==============================] - 24s 180us/step - loss: 0.2147 - acc: 0.9127 - val_loss: 0.1733 - val_acc: 0.9489\n",
      " — val_f1_micro: 0.9489 — val_precision_micro: 0.9489 — val_recall_micro: 0.9489\n",
      " — val_f1_macro: 0.4644 — val_precision_macro: 0.7154 — val_recall_macro: 0.4284\n",
      " — val_f1_weighted: 0.9489 — val_precision_weighted: 0.9489 — val_recall_weighted: 0.9489\n",
      "Epoch 7/30\n",
      "133227/133227 [==============================] - 25s 187us/step - loss: 0.2123 - acc: 0.9139 - val_loss: 0.1667 - val_acc: 0.9513\n",
      " — val_f1_micro: 0.9513 — val_precision_micro: 0.9513 — val_recall_micro: 0.9513\n",
      " — val_f1_macro: 0.5240 — val_precision_macro: 0.7115 — val_recall_macro: 0.4718\n",
      " — val_f1_weighted: 0.9513 — val_precision_weighted: 0.9513 — val_recall_weighted: 0.9513\n",
      "Epoch 8/30\n",
      "133227/133227 [==============================] - 25s 191us/step - loss: 0.2120 - acc: 0.9141 - val_loss: 0.1646 - val_acc: 0.9483\n",
      " — val_f1_micro: 0.9483 — val_precision_micro: 0.9483 — val_recall_micro: 0.9483\n",
      " — val_f1_macro: 0.4476 — val_precision_macro: 0.7302 — val_recall_macro: 0.4118\n",
      " — val_f1_weighted: 0.9483 — val_precision_weighted: 0.9483 — val_recall_weighted: 0.9483\n",
      "Epoch 9/30\n",
      "133227/133227 [==============================] - 26s 194us/step - loss: 0.2092 - acc: 0.9147 - val_loss: 0.1675 - val_acc: 0.9461\n",
      " — val_f1_micro: 0.9461 — val_precision_micro: 0.9461 — val_recall_micro: 0.9461\n",
      " — val_f1_macro: 0.4781 — val_precision_macro: 0.6807 — val_recall_macro: 0.4305\n",
      " — val_f1_weighted: 0.9461 — val_precision_weighted: 0.9461 — val_recall_weighted: 0.9461\n",
      "Epoch 10/30\n",
      "133227/133227 [==============================] - 26s 194us/step - loss: 0.2093 - acc: 0.9153 - val_loss: 0.1702 - val_acc: 0.9502\n",
      " — val_f1_micro: 0.9502 — val_precision_micro: 0.9502 — val_recall_micro: 0.9502\n",
      " — val_f1_macro: 0.5337 — val_precision_macro: 0.7481 — val_recall_macro: 0.4798\n",
      " — val_f1_weighted: 0.9502 — val_precision_weighted: 0.9502 — val_recall_weighted: 0.9502\n",
      "Epoch 11/30\n",
      "133227/133227 [==============================] - 27s 203us/step - loss: 0.2077 - acc: 0.9160 - val_loss: 0.1698 - val_acc: 0.9473\n",
      " — val_f1_micro: 0.9473 — val_precision_micro: 0.9473 — val_recall_micro: 0.9473\n",
      " — val_f1_macro: 0.4947 — val_precision_macro: 0.6986 — val_recall_macro: 0.4473\n",
      " — val_f1_weighted: 0.9473 — val_precision_weighted: 0.9473 — val_recall_weighted: 0.9473\n",
      "Epoch 12/30\n",
      "133227/133227 [==============================] - 25s 191us/step - loss: 0.2073 - acc: 0.9164 - val_loss: 0.1623 - val_acc: 0.9479\n",
      " — val_f1_micro: 0.9479 — val_precision_micro: 0.9479 — val_recall_micro: 0.9479\n",
      " — val_f1_macro: 0.4827 — val_precision_macro: 0.6314 — val_recall_macro: 0.4419\n",
      " — val_f1_weighted: 0.9479 — val_precision_weighted: 0.9479 — val_recall_weighted: 0.9479\n",
      "Epoch 13/30\n",
      "133227/133227 [==============================] - 25s 185us/step - loss: 0.2052 - acc: 0.9176 - val_loss: 0.1676 - val_acc: 0.9477\n",
      " — val_f1_micro: 0.9477 — val_precision_micro: 0.9477 — val_recall_micro: 0.9477\n",
      " — val_f1_macro: 0.4815 — val_precision_macro: 0.6977 — val_recall_macro: 0.4431\n",
      " — val_f1_weighted: 0.9477 — val_precision_weighted: 0.9477 — val_recall_weighted: 0.9477\n",
      "Epoch 14/30\n",
      "133227/133227 [==============================] - 25s 190us/step - loss: 0.2047 - acc: 0.9183 - val_loss: 0.1707 - val_acc: 0.9501\n",
      " — val_f1_micro: 0.9501 — val_precision_micro: 0.9501 — val_recall_micro: 0.9501\n",
      " — val_f1_macro: 0.5204 — val_precision_macro: 0.6773 — val_recall_macro: 0.4786\n",
      " — val_f1_weighted: 0.9501 — val_precision_weighted: 0.9501 — val_recall_weighted: 0.9501\n",
      "Epoch 15/30\n",
      "133227/133227 [==============================] - 26s 194us/step - loss: 0.2047 - acc: 0.9185 - val_loss: 0.1675 - val_acc: 0.9475\n",
      " — val_f1_micro: 0.9475 — val_precision_micro: 0.9475 — val_recall_micro: 0.9475\n",
      " — val_f1_macro: 0.5091 — val_precision_macro: 0.6479 — val_recall_macro: 0.4601\n",
      " — val_f1_weighted: 0.9475 — val_precision_weighted: 0.9475 — val_recall_weighted: 0.9475\n",
      "Epoch 16/30\n",
      "133227/133227 [==============================] - 24s 183us/step - loss: 0.2049 - acc: 0.9187 - val_loss: 0.1666 - val_acc: 0.9503\n",
      " — val_f1_micro: 0.9503 — val_precision_micro: 0.9503 — val_recall_micro: 0.9503\n",
      " — val_f1_macro: 0.4836 — val_precision_macro: 0.6223 — val_recall_macro: 0.4408\n",
      " — val_f1_weighted: 0.9503 — val_precision_weighted: 0.9503 — val_recall_weighted: 0.9503\n",
      "Epoch 17/30\n",
      "133227/133227 [==============================] - 25s 184us/step - loss: 0.2032 - acc: 0.9188 - val_loss: 0.1805 - val_acc: 0.9474\n",
      " — val_f1_micro: 0.9474 — val_precision_micro: 0.9474 — val_recall_micro: 0.9474\n",
      " — val_f1_macro: 0.5336 — val_precision_macro: 0.6748 — val_recall_macro: 0.4928\n",
      " — val_f1_weighted: 0.9474 — val_precision_weighted: 0.9474 — val_recall_weighted: 0.9474\n",
      "Epoch 18/30\n",
      "133227/133227 [==============================] - 25s 187us/step - loss: 0.2041 - acc: 0.9186 - val_loss: 0.1794 - val_acc: 0.9442\n",
      " — val_f1_micro: 0.9442 — val_precision_micro: 0.9442 — val_recall_micro: 0.9442\n",
      " — val_f1_macro: 0.5345 — val_precision_macro: 0.6259 — val_recall_macro: 0.4956\n",
      " — val_f1_weighted: 0.9442 — val_precision_weighted: 0.9442 — val_recall_weighted: 0.9442\n",
      "Epoch 19/30\n",
      "133227/133227 [==============================] - 26s 192us/step - loss: 0.2030 - acc: 0.9192 - val_loss: 0.1669 - val_acc: 0.9487\n",
      " — val_f1_micro: 0.9487 — val_precision_micro: 0.9487 — val_recall_micro: 0.9487\n",
      " — val_f1_macro: 0.5162 — val_precision_macro: 0.7020 — val_recall_macro: 0.4653\n",
      " — val_f1_weighted: 0.9487 — val_precision_weighted: 0.9487 — val_recall_weighted: 0.9487\n",
      "Epoch 20/30\n",
      "133227/133227 [==============================] - 25s 186us/step - loss: 0.2025 - acc: 0.9190 - val_loss: 0.1673 - val_acc: 0.9505\n",
      " — val_f1_micro: 0.9505 — val_precision_micro: 0.9505 — val_recall_micro: 0.9505\n",
      " — val_f1_macro: 0.5317 — val_precision_macro: 0.6938 — val_recall_macro: 0.4855\n",
      " — val_f1_weighted: 0.9505 — val_precision_weighted: 0.9505 — val_recall_weighted: 0.9505\n",
      "Epoch 21/30\n",
      "133227/133227 [==============================] - 25s 188us/step - loss: 0.2020 - acc: 0.9199 - val_loss: 0.1624 - val_acc: 0.9513\n",
      " — val_f1_micro: 0.9513 — val_precision_micro: 0.9513 — val_recall_micro: 0.9513\n",
      " — val_f1_macro: 0.4666 — val_precision_macro: 0.7529 — val_recall_macro: 0.4260\n",
      " — val_f1_weighted: 0.9513 — val_precision_weighted: 0.9513 — val_recall_weighted: 0.9513\n",
      "Epoch 22/30\n",
      "133227/133227 [==============================] - 26s 193us/step - loss: 0.2010 - acc: 0.9200 - val_loss: 0.1724 - val_acc: 0.9496\n",
      " — val_f1_micro: 0.9496 — val_precision_micro: 0.9496 — val_recall_micro: 0.9496\n",
      " — val_f1_macro: 0.5127 — val_precision_macro: 0.6535 — val_recall_macro: 0.4729\n",
      " — val_f1_weighted: 0.9496 — val_precision_weighted: 0.9496 — val_recall_weighted: 0.9496\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133227/133227 [==============================] - 25s 188us/step - loss: 0.2011 - acc: 0.9199 - val_loss: 0.1696 - val_acc: 0.9506\n",
      " — val_f1_micro: 0.9506 — val_precision_micro: 0.9506 — val_recall_micro: 0.9506\n",
      " — val_f1_macro: 0.5194 — val_precision_macro: 0.6907 — val_recall_macro: 0.4716\n",
      " — val_f1_weighted: 0.9506 — val_precision_weighted: 0.9506 — val_recall_weighted: 0.9506\n",
      "Epoch 24/30\n",
      "133227/133227 [==============================] - 26s 192us/step - loss: 0.2007 - acc: 0.9206 - val_loss: 0.1685 - val_acc: 0.9501\n",
      " — val_f1_micro: 0.9501 — val_precision_micro: 0.9501 — val_recall_micro: 0.9501\n",
      " — val_f1_macro: 0.5322 — val_precision_macro: 0.7093 — val_recall_macro: 0.4794\n",
      " — val_f1_weighted: 0.9501 — val_precision_weighted: 0.9501 — val_recall_weighted: 0.9501\n",
      "Epoch 25/30\n",
      "133227/133227 [==============================] - 24s 184us/step - loss: 0.1999 - acc: 0.9214 - val_loss: 0.1596 - val_acc: 0.9522\n",
      " — val_f1_micro: 0.9522 — val_precision_micro: 0.9522 — val_recall_micro: 0.9522\n",
      " — val_f1_macro: 0.5499 — val_precision_macro: 0.7567 — val_recall_macro: 0.4880\n",
      " — val_f1_weighted: 0.9522 — val_precision_weighted: 0.9522 — val_recall_weighted: 0.9522\n",
      "Epoch 26/30\n",
      "133227/133227 [==============================] - 25s 188us/step - loss: 0.1989 - acc: 0.9207 - val_loss: 0.1598 - val_acc: 0.9526\n",
      " — val_f1_micro: 0.9526 — val_precision_micro: 0.9526 — val_recall_micro: 0.9526\n",
      " — val_f1_macro: 0.5151 — val_precision_macro: 0.7214 — val_recall_macro: 0.4639\n",
      " — val_f1_weighted: 0.9526 — val_precision_weighted: 0.9526 — val_recall_weighted: 0.9526\n",
      "Epoch 27/30\n",
      "133227/133227 [==============================] - 24s 183us/step - loss: 0.1979 - acc: 0.9217 - val_loss: 0.1621 - val_acc: 0.9506\n",
      " — val_f1_micro: 0.9506 — val_precision_micro: 0.9506 — val_recall_micro: 0.9506\n",
      " — val_f1_macro: 0.5118 — val_precision_macro: 0.6987 — val_recall_macro: 0.4652\n",
      " — val_f1_weighted: 0.9506 — val_precision_weighted: 0.9506 — val_recall_weighted: 0.9506\n",
      "Epoch 28/30\n",
      "133227/133227 [==============================] - 25s 184us/step - loss: 0.1991 - acc: 0.9210 - val_loss: 0.1632 - val_acc: 0.9500\n",
      " — val_f1_micro: 0.9500 — val_precision_micro: 0.9500 — val_recall_micro: 0.9500\n",
      " — val_f1_macro: 0.4973 — val_precision_macro: 0.8548 — val_recall_macro: 0.4434\n",
      " — val_f1_weighted: 0.9500 — val_precision_weighted: 0.9500 — val_recall_weighted: 0.9500\n",
      "Epoch 29/30\n",
      "133227/133227 [==============================] - 25s 187us/step - loss: 0.1978 - acc: 0.9217 - val_loss: 0.1582 - val_acc: 0.9495\n",
      " — val_f1_micro: 0.9495 — val_precision_micro: 0.9495 — val_recall_micro: 0.9495\n",
      " — val_f1_macro: 0.4754 — val_precision_macro: 0.7178 — val_recall_macro: 0.4297\n",
      " — val_f1_weighted: 0.9495 — val_precision_weighted: 0.9495 — val_recall_weighted: 0.9495\n",
      "Epoch 30/30\n",
      "133227/133227 [==============================] - 25s 187us/step - loss: 0.1974 - acc: 0.9219 - val_loss: 0.1600 - val_acc: 0.9491\n",
      " — val_f1_micro: 0.9491 — val_precision_micro: 0.9491 — val_recall_micro: 0.9491\n",
      " — val_f1_macro: 0.4952 — val_precision_macro: 0.7443 — val_recall_macro: 0.4395\n",
      " — val_f1_weighted: 0.9491 — val_precision_weighted: 0.9491 — val_recall_weighted: 0.9491\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "batch_size = 16\n",
    "\n",
    "history = model.fit(X_tr, Y_tr_nn, \n",
    "                    epochs=epochs, \n",
    "                    shuffle=True, verbose=1, \n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(X_vl, Y_vl_nn),\n",
    "                    callbacks=[metrics_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max f1 micro: 0.9526 epoch: 25\n",
      "max f1 macro: 0.5499 epoch: 24\n"
     ]
    }
   ],
   "source": [
    "# max micro and macro f1 score obtained\n",
    "f1_micro = metrics_callback.val_f1s_micro\n",
    "f1_macro = metrics_callback.val_f1s_macro\n",
    "\n",
    "print(\"max f1 micro:\", np.round(np.max(f1_micro), 4), \"epoch:\", np.argmax(f1_micro))\n",
    "print(\"max f1 macro:\", np.round(np.max(f1_macro), 4), \"epoch:\", np.argmax(f1_macro))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='training loss', linestyle='--', marker='o')\n",
    "plt.plot(history.history['val_loss'], label='validation loss', linestyle='-', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Categorical cross-entropy')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('../draft_presentation/images/loss.png')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['acc'], label='training accuracy', linestyle='--', marker='o')\n",
    "plt.plot(history.history['val_acc'], label='validation accuracy', linestyle='-', marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('../draft_presentation/images/accuracy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of F1-score, Precision and Recall (micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(metrics_callback.val_f1s_micro, label='f1-score', marker='o', alpha=0.7)\n",
    "plt.plot(metrics_callback.val_precisions_micro, label='precision', marker='D', alpha=0.7)\n",
    "plt.plot(metrics_callback.val_recalls_micro, label='recall', marker='*', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Micro-averaged performance')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('../draft_presentation/images/micro.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of F1-score, Precision and Recall (macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(metrics_callback.val_f1s_macro, label='f1-score', marker='o', alpha=0.7)\n",
    "plt.plot(metrics_callback.val_precisions_macro, label='precision', marker='D', alpha=0.7)\n",
    "plt.plot(metrics_callback.val_recalls_macro, label='recall', marker='*', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Macro-averaged performance')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('../draft_presentation/images/macro.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of F1-score, Precision and Recall (weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(metrics_callback.val_f1s_weighted, label='f1-score', marker='o', alpha=0.7)\n",
    "plt.plot(metrics_callback.val_precisions_weighted, label='precision', marker='D', alpha=0.7)\n",
    "plt.plot(metrics_callback.val_recalls_weighted, label='recall', marker='*', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Weighted-averaged performance')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('../draft_presentation/images/weighted.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "IdSentence|startOffset-endOffset|text|null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token_spans(txt):\n",
    "    token_offset = []\n",
    "    tokens = nltk.word_tokenize(txt)\n",
    "    offset = 0\n",
    "    for token in tokens:\n",
    "        offset = txt.find(token, offset)\n",
    "        token_offset.append((token, offset, offset+len(token)-1))\n",
    "        offset += len(token)\n",
    "    return tokens, token_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_end_entity_index(labels_list, current_word_index):\n",
    "    end_entity_index = current_word_index\n",
    "    for i in range(current_word_index + 1, len(labels_list)):\n",
    "        if labels_list[i] == 1: # if label == I\n",
    "            end_entity_index += 1\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    return end_entity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = EnglishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_with_POS(sentences, pos=True, stem=True):\n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    tokenized_pos = pos_tag(tokenized_sentences, tagset=None)\n",
    "    \n",
    "    if pos is False and stem is False:\n",
    "        print('original')\n",
    "        return tokenized_sentences\n",
    "    if stem and pos:\n",
    "        print('stem + pos')\n",
    "        tokenized_pos = [ [stemmer.stem(w) + '_' + pos for w, pos in s ] for s in tokenized_pos]\n",
    "    if stem and pos is False:\n",
    "        print('stem')\n",
    "        tokenized_pos = [ [stemmer.stem(w) for w in s ] for s in tokenized_sentences]\n",
    "    else:\n",
    "        print('pos')\n",
    "        tokenized_pos = [ [w + '_' + pos for w, pos in s ] for s in tokenized_pos]\n",
    "    return tokenized_pos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_string = ''\n",
    "vector_size = 20\n",
    "\n",
    "\n",
    "for index, row in sentences_df_test1.iterrows():\n",
    "    sentenceId = row['sentenceID']\n",
    "    sentenceText = row['sentenceText']\n",
    "    # 1. tokenize sentence\n",
    "    tok_sentence, token_offset = token_spans(sentenceText)\n",
    "    # 2. add part of speech\n",
    "    #tok_sentence_pos = [ word + '_' + pos for word, pos in pos_tag(tok_sentence, tagset=None)]\n",
    "    tok_sentence_pos = [ stemmer.stem(word) for word, pos in pos_tag(tok_sentence, tagset=None)]\n",
    "    # 3. get word vectors, predict and write output line\n",
    "    vectors_to_predict = np.array([]).reshape(0, vector_size)\n",
    "    for word in tok_sentence_pos:\n",
    "        vector = word_vectors[word]\n",
    "        vectors_to_predict = np.vstack((vectors_to_predict, vector))\n",
    "    # 4. predict\n",
    "    predictions = model.predict(vectors_to_predict)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    # 5. generate output\n",
    "    for i in range(len(predicted_labels)):\n",
    "        if predicted_labels[i] == 0:\n",
    "            end_entity_index = find_end_entity_index(predicted_labels, i)\n",
    "            start = token_offset[i][1]\n",
    "            end = token_offset[end_entity_index][2]\n",
    "            output_string += sentenceId + '|' + str(start) + '-' + str(end) + '|' + sentenceText[start:end+1] + '|null\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_file_task_1 = '../results/task9.1_GROUP_1.txt'\n",
    "with open(output_file_task_1, \"w\") as out_file:\n",
    "    out_file.write(output_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
